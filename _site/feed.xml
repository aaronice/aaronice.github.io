<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Code Is Not Cold</title>
    <description>Blog on technology, life, thinking, etc.</description>
    <link>aaronice.github.io/</link>
    <atom:link href="aaronice.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 28 Feb 2016 11:07:41 -0800</pubDate>
    <lastBuildDate>Sun, 28 Feb 2016 11:07:41 -0800</lastBuildDate>
    <generator>Jekyll v3.1.1</generator>
    
      <item>
        <title>Postgre Sql Node</title>
        <description>
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://mherman.org/blog/2015/02/12/postgresql-and-nodejs/#.VtH2-cddhNE&quot;&gt;PostgreSQL and NodeJS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 28 Feb 2016 00:00:00 -0800</pubDate>
        <link>aaronice.github.io/2016/02/28/postgre-sql-node/</link>
        <guid isPermaLink="true">aaronice.github.io/2016/02/28/postgre-sql-node/</guid>
        
        
      </item>
    
      <item>
        <title>Full Text Search Mongodb</title>
        <description>
&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.compose.io/articles/full-text-search-with-mongodb-and-node-js/&quot;&gt;Full Text Search with MongoDB &amp;amp; Node.js&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://code.tutsplus.com/tutorials/full-text-search-in-mongodb--cms-24835&quot;&gt;Full-Text Search in MongoDB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 27 Feb 2016 00:00:00 -0800</pubDate>
        <link>aaronice.github.io/2016/02/27/full-text-search-mongodb/</link>
        <guid isPermaLink="true">aaronice.github.io/2016/02/27/full-text-search-mongodb/</guid>
        
        
      </item>
    
      <item>
        <title>MongoDB vs CouchDB</title>
        <description>&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.nahurst.com/visual-guide-to-nosql-systems&quot;&gt;Visual Guide to NoSQL Systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis&quot;&gt;Cassandra vs MongoDB vs CouchDB vs Redis vs Riak vs HBase vs Couchbase vs OrientDB vs Aerospike vs Neo4j vs Hypertable vs ElasticSearch vs Accumulo vs VoltDB vs Scalaris vs RethinkDB comparison&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.scottlogic.com/2014/08/04/mongodb-vs-couchdb.html&quot;&gt;MongoDB vs CouchDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;openmymind.net/2011/10/27/A-MongoDB-Guy-Learns-CouchDB/&quot;&gt;A MongoDB Guy Learns CouchDB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 26 Feb 2016 04:00:00 -0800</pubDate>
        <link>aaronice.github.io/2016/02/26/mongodb-couchdb/</link>
        <guid isPermaLink="true">aaronice.github.io/2016/02/26/mongodb-couchdb/</guid>
        
        
      </item>
    
      <item>
        <title>Safe password storage in Node.js</title>
        <description>&lt;h2 id=&quot;why-use-bcrypt&quot;&gt;Why Use &lt;em&gt;bcrypt&lt;/em&gt;?&lt;/h2&gt;

&lt;h2 id=&quot;why-not-use-md5-sha1-sha256-sha512-sha-3-etc&quot;&gt;Why Not Use MD5, SHA1, SHA256, SHA512, SHA-3, etc?&lt;/h2&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://codahale.com/how-to-safely-store-a-password/&quot;&gt;How To Safely Store A Password&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nakedsecurity.sophos.com/2013/11/20/serious-security-how-to-store-your-users-passwords-safely/&quot;&gt;Serious Security: How to store your users’ passwords safely&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Bcrypt&quot;&gt;bcrypt from Wikipedia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ncb000gt/node.bcrypt.js&quot;&gt;bcrypt for NodeJs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;more&quot;&gt;More&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://nodegoat.herokuapp.com/tutorial&quot;&gt;OWASP Node Goat Tutorial: Fixing OWASP Top 10&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.risingstack.com/node-js-security-checklist/&quot;&gt;Node.js Security Checklist&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nodesecurity.io/&quot;&gt;Node Security Project&lt;/a&gt; &lt;a href=&quot;https://nodesecurity.io/resources&quot;&gt;/resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 25 Feb 2016 04:00:00 -0800</pubDate>
        <link>aaronice.github.io/2016/02/25/safely-store-password-node/</link>
        <guid isPermaLink="true">aaronice.github.io/2016/02/25/safely-store-password-node/</guid>
        
        
      </item>
    
      <item>
        <title>Clustering in Node.js</title>
        <description>&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.carbonfive.com/2014/02/28/taking-advantage-of-multi-processor-environments-in-node-js/&quot;&gt;Taking Advantage of Multi-Processor Environments in Node.js&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 24 Feb 2016 04:00:00 -0800</pubDate>
        <link>aaronice.github.io/2016/02/24/node-js-cluster/</link>
        <guid isPermaLink="true">aaronice.github.io/2016/02/24/node-js-cluster/</guid>
        
        
      </item>
    
      <item>
        <title>ES6 Learning</title>
        <description>&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ericdouglas/ES6-Learning&quot;&gt;ECMAScript 6 Learning - List of resources to learn ECMAScript 6&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ccoenraets.github.io/es6-tutorial/&quot;&gt;ECMAScript 2015 (ES6) Tutorial with Babel 6 and Webpack&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://gank.io/post/564151c1f1df1210001c9161&quot;&gt;给 JavaScript 初心者的 ES2015 实战&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 20 Feb 2016 04:00:00 -0800</pubDate>
        <link>aaronice.github.io/2016/02/20/es6-features/</link>
        <guid isPermaLink="true">aaronice.github.io/2016/02/20/es6-features/</guid>
        
        
      </item>
    
      <item>
        <title>React Performance Optimization</title>
        <description>&lt;p&gt;React Performance Optimization&lt;/p&gt;

&lt;p&gt;(To do)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://facebook.github.io/react/docs/perf.html&quot;&gt;Facebook React Performance Tools&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://facebook.github.io/react/docs/advanced-performance.html&quot;&gt;React Advanced Performance&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[Optimizing React Performance using keys, component life cycle, and performance tools&lt;/td&gt;
          &lt;td&gt;Part 1](http://jaero.space/blog/react-performance-1)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kelle.co/react-perf-slides/#1&quot;&gt;Jaero Slides&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jonmiles.github.io/react-performance-tests/react.html&quot;&gt;React.js Performance Tests&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@joaomilho/arguing-for-reactjs-7b80aafc6493&quot;&gt;Arguing for ReactJS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 18 Feb 2016 04:00:00 -0800</pubDate>
        <link>aaronice.github.io/2016/02/18/react-performance-optimization/</link>
        <guid isPermaLink="true">aaronice.github.io/2016/02/18/react-performance-optimization/</guid>
        
        
      </item>
    
      <item>
        <title>Anti Anti-spider Strategy</title>
        <description>&lt;h1 id=&quot;the-anti-anti-scraping-strategy&quot;&gt;The Anti Anti-scraping Strategy&lt;/h1&gt;

&lt;p&gt;Before knowing how to prevent being blacklisted while scraping, we need to understand how a website detect web crawlers.&lt;/p&gt;

&lt;p&gt;There’re multiple anti-scraping mechanisms to distinguish a spider from a normal user. Some of the methods are&lt;a href=&quot;https://learn.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;Unusual traffic/high download &lt;strong&gt;rate&lt;/strong&gt; especially from a &lt;strong&gt;single&lt;/strong&gt; client/or IP address within a short &lt;strong&gt;time&lt;/strong&gt; span.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;&lt;strong&gt;Repetitive&lt;/strong&gt; tasks performed on the website – based on an assumption that a human user won’t perform the same repetitive tasks all the time.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Detection through &lt;strong&gt;honeypots&lt;/strong&gt; – these honeypots are usually links which &lt;strong&gt;aren’t visible&lt;/strong&gt; to a normal user but only to a spider. When a scraper/spider tries to access the link, the alarms are tripped.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Corresponding to the anti-scraping mechanisms are the following strategies:&lt;/p&gt;

&lt;h2 id=&quot;ip-addresses-rotation&quot;&gt;IP Addresses Rotation&lt;/h2&gt;

&lt;p&gt;IP blacklisting is the perhaps the easiest way of anti-scraping. By creating a pool of IP addresses, and making requests using different IP, will make it harder for server to detect crawlers.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use a list of IP from proxy services
    &lt;ul&gt;
      &lt;li&gt;Randomly pick an IP for certain time interval&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cookie-rotation&quot;&gt;Cookie Rotation&lt;/h2&gt;
&lt;p&gt;Cookies are encrypted data stored in client side, some website use cookie to identify user. If a user client sends requests in high frequency, it’s possible to be identified as suspicious crawler, thus denied access.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Customize and manage a pool of cookies
    &lt;ul&gt;
      &lt;li&gt;Sending request to server without cookie headers, parse the package for set-cookie value; store it in cookie collector;&lt;/li&gt;
      &lt;li&gt;Get cookie from cookie collect and if the cookie is not usable, then delete it from cookie collector;&lt;/li&gt;
      &lt;li&gt;Manage the cookie collector with timestamp, in order for the spider to get the oldest cookie first each time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Disable cookie
    &lt;ul&gt;
      &lt;li&gt;By disabling cookie, it may help prevent being banned by some websites which use cookie to identify crawlers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;user-agent-imitation&quot;&gt;User-Agent Imitation&lt;/h2&gt;

&lt;p&gt;To disguise as browser, one of the approaches is to modify the User-Agent. User-Agent is a string in request header that contains user-agent information, such as the version of web browser, client, operating system etc.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spoof the User-Agent by making a list of user agents and randomly pick one for each request. Setting the User-Agent to a common web
browser instead of using the default ones.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;rate-limit&quot;&gt;Rate Limit&lt;/h2&gt;

&lt;p&gt;Slow down the crawling, treat the websites nicely, don’t overwhelm it, or DDoS the server.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Put some random sleeps between requests&lt;/li&gt;
  &lt;li&gt;Add some delays after crawling a number of pages&lt;/li&gt;
  &lt;li&gt;Use the lowest number of concurrent requests possible&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;be-careful-about-honeypots&quot;&gt;Be Careful About Honeypots&lt;/h2&gt;

&lt;p&gt;These honeypots usually are links that normal user can’t see but a spider can&lt;a href=&quot;https://learn.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;. They might be having the CSS style “display:none”. Thus the detection of honeypots can be tricky.&lt;/p&gt;

&lt;h2 id=&quot;avoid-repetitive-crawling-pattern&quot;&gt;Avoid Repetitive Crawling Pattern&lt;/h2&gt;

&lt;p&gt;Some websites implemented intelligent anti-crawling mechanisms, thus a repetitive action will be likely detected as spider. To make a spider looks like a human, add random clicks, mouse movement, random actions etc. Using automated testing tool like Selenium may simulator normal “human actions”.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://learn.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/&quot;&gt;HOW TO PREVENT GETTING BLACKLISTED WHILE SCRAPING&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.dianacody.com/2014/10/01/spider_5.html&quot;&gt;防止爬虫被墙的方法总结&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://medium.com/@Masutangu/%E5%BA%94%E5%AF%B9%E5%8F%8D%E7%88%AC%E8%99%AB%E4%B9%8B%E6%8D%A2cookie-d3b48b02d0e6&quot;&gt;应对反爬虫之换Cookie&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 15 Feb 2016 04:00:00 -0800</pubDate>
        <link>aaronice.github.io/2016/02/15/anti-anti-spider-strategy/</link>
        <guid isPermaLink="true">aaronice.github.io/2016/02/15/anti-anti-spider-strategy/</guid>
        
        
      </item>
    
      <item>
        <title>Modern Software Project Management - Agile Methodology</title>
        <description>&lt;p&gt;Individuals and Interactions » Processes and Tools
Working Software » comprehensive documentation
Customer Collaboration » Contract Negotiation
Responding to Change » Following a Plan&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://agilemethodology.org&quot;&gt;Agile Methodology&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 11 Feb 2016 04:00:00 -0800</pubDate>
        <link>aaronice.github.io/2016/02/11/agile-methodology/</link>
        <guid isPermaLink="true">aaronice.github.io/2016/02/11/agile-methodology/</guid>
        
        
      </item>
    
      <item>
        <title>What on earth is SOA?</title>
        <description>&lt;p&gt;To fill.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Service-oriented_architecture&quot;&gt;Wiki Service-oriented_architecture&lt;/a&gt;
# &lt;a href=&quot;http://aosabook.org/en/index.html&quot;&gt;The Architecture of Open Source Applications&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 09 Feb 2016 04:00:00 -0800</pubDate>
        <link>aaronice.github.io/2016/02/09/service-oriented-architecture/</link>
        <guid isPermaLink="true">aaronice.github.io/2016/02/09/service-oriented-architecture/</guid>
        
        
      </item>
    
  </channel>
</rss>
